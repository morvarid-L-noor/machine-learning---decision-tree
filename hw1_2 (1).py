# -*- coding: utf-8 -*-
"""HW1-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wemeFi4RrMEYiuNRrB64eUyx-eFYljzC
"""

import pandas as pd
import random
import numpy as np
from pprint import pprint

def entropy(target):
    elements,counts = np.unique(target,returnc = True)
    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])
    return entropy

def Information-gain(data,attribute_name,target_name="target"):
    
    # entropy ofdataset
    total_entropy = entropy(data[target_name])
    
    #each value and its count in dataset
    vals,counts= np.unique(data[attribute_name],returnc=True)
    
    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])
    
    Information_Gain = total_entropy - Weighted_Entropy
    return Information_Gain

def id3(data,raw-data,features,target_attribute_name="target",parent_node_class = None):
    
    #If one of this is satisfied, we  return a leaf node
    
    #If all target_values are the same , return this value
    if len(np.unique(data[target_attribute_name])) <= 1:
        return np.unique(data[target_attribute_name])[0]
    
    elif len(data)==0:
        return np.unique(raw-data[target_attribute_name])[np.argmax(np.unique(raw-data[target_attribute_name],returnc=True)[1])]
    
    elif len(features) ==0:
        return parent_node_class
    
    #otherwise , grow the tree!
    
    else:
        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],returnc=True)[1])]
        
        #Select the feature which best splits the dataset
        item_values = [Information-gain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset
        best_feature_index = np.argmax(item_values)
        best_feature = features[best_feature_index]
        
        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information
        #gain in the first run
        tree = {best_feature:{}}
        
        
        #Remove the feature with the best inforamtion gain from the feature space
        features = [i for i in features if i != best_feature]
        
        for value in np.unique(data[best_feature]):
            value = value
            #split the dataset along the value of the feature with the largest information gain and  create sub_datasets
            sub_data = data.where(data[best_feature] == value).dropna()
            
            #recursive call
            subtree = id3(sub_data,ds,features,target_attribute_name,parent_node_class)
            
            #Add the sub tree, grown from the sub_dataset to the tree under the root node
            tree[best_feature][value] = subtree
            
        return(tree)

def predict(queue,tree,default = 1):
 for key in list(queue.keys()):
        if key in list(tree.keys()):
           
            try:
                result = tree[key][queue[key]] 
            except:
                return default
  
            result = tree[key][queue[key]]

            if isinstance(result,dict):
                return predict(queue,result)

            else:
                return result

filename = "train.csv"
ds = pd.read_csv(filename)
ds = ds.sample(frac=1)  #choose some rows randomly from dataset ( we can change "frac" to specify the range )

training_data = ds
validateing_data = pd.read_csv("adult.validate.10k.csv") 

def validate(data,tree):
    #removing the target feature column from the original dataset and 
    #convert it to a dictionary and  creating  new queue instances 
    queries = data.iloc[:,:-1].to_dict(orient = "records")
    
    #Creating a empty DataFrame 
    predicted = pd.DataFrame(columns=["predicted"]) 
    
    #caculating the accuracy
    for i in range(len(data)):
        predicted.loc[i,"predicted"] = predict(queries[i],tree,1.0) 
    print('The prediction accuracy is: ',(np.sum(predicted["predicted"] == data["target"])/len(data))*100,'%')

tree = id3(ds,ds,ds.columns[1:])
pprint(tree)
validate(validateing_data,tree)

#-----------------------------------------------------------------------------------------------------------------------------Q2-a

from sklearn.tree import DecisionTreeClassifier
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

data = pd.read_csv('train.csv')
X,y = data[[ 'workclass',  'education', 'marital-status', 'occupation','relationship',  'race','sex','native-country']],data['target']
X,y = X.fillna(0),y.fillna(0)
X = pd.get_dummies(X)    # random_state=1
X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.25,shuffle = True)

model = DecisionTreeClassifier(random_state=1).fit(X_train,y_train)
y_predicted = model.predict(X_validate)
print('Training accuracy: ',model.score(X_train,y_train))
print('validation Accuracy: ',model.score(X_validate,y_validate))

path=DecisionTreeClassifier(random_state=1).\
cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")

clfs = []
for ccp_alpha in ccp_alphas:
  if(ccp_alpha >= 0):
    clf = DecisionTreeClassifier(random_state=1,ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

print("Number of nodes in the last tree is: {} with ccp_alpha: {} and a depth of: {}".format(
clfs[-1].tree_.node_count, ccp_alphas[-1],clfs[-1].tree_.max_depth))

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]
node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas[:-1], node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas[:-1], depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

train_scores = [clf.score(X_train, y_train) for clf in clfs]
validate_scores = [clf.score(X_validate, y_validate) for clf in clfs]
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and validateing sets")
ax.plot(ccp_alphas[:-1], train_scores, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas[:-1], validate_scores, marker='o', label="validate",
        drawstyle="steps-post")
ax.legend()
plt.show()

index_best_model = np.argmax(validate_scores)
best_model = clfs[index_best_model]
print('Training accuracy of best model: ',best_model.score(X_train, y_train))
print('validate accuracy of best model: ',best_model.score(X_validate, y_validate))

# ---------------------------------------------------------------------------------------------------------------------------Q2-b.1

import pandas
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVR
import numpy as np

data = pandas.read_csv('train.csv')
test = pandas.read_csv('adult.test.10k.csv')
X_tst,y_tst = test[[ 'workclass',  'education', 'marital-status', 'occupation','relationship',  'race','sex','native-country']],test['target']
X,y = data[[ 'workclass',  'education', 'marital-status', 'occupation','relationship',  'race','sex','native-country']],data['target']
X,y = X.fillna(0),y.fillna(0)
X_tst,y_tst = X_tst.fillna(0),y_tst.fillna(0)
X = pandas.get_dummies(X)  
y = pandas.get_dummies(y)
X_tst = pandas.get_dummies(X_tst)  
y_tst = pandas.get_dummies(y_tst)

scaler = MinMaxScaler(feature_range=(0, 1))
X = scaler.fit_transform(X)
X_tst = scaler.fit_transform(X_tst)

scores = []
best_svr = SVR(kernel='rbf')
cv = KFold(n_splits=4, shuffle=False)

best_svr.fit(X,y[">50K"])
scores.append(best_svr.score(X_test, y_test[">50K"]))

y_test.head

print(np.mean(scores))

data = pd.read_csv('train.csv')
X,y = data[[ 'workclass',  'education', 'marital-status', 'occupation','relationship',  'race','sex','native-country']],data['target']
X,y = X.fillna(0),y.fillna(0)
X = pd.get_dummies(X)    # random_state=1
X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.25,shuffle = False)

y_validate.head

model = DecisionTreeClassifier(random_state=1).fit(X_train,y_train)
y_predicted = model.predict(X_validate)
print('Training accuracy: ',model.score(X_train,y_train))
print('validation Accuracy: ',model.score(X_validate,y_validate))

path=DecisionTreeClassifier(random_state=1).\
cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")

clfs = []
for ccp_alpha in ccp_alphas:
  if(ccp_alpha >= 0):
    clf = DecisionTreeClassifier(random_state=1,ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

print("Number of nodes in the last tree is: {} with ccp_alpha: {} and a depth of: {}".format(
clfs[-1].tree_.node_count, ccp_alphas[-1],clfs[-1].tree_.max_depth))

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]
node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

train_scores = [clf.score(X_train, y_train) for clf in clfs]
validate_scores = [clf.score(X_validate, y_validate) for clf in clfs]
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and validateing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas, validate_scores, marker='o', label="validate",
        drawstyle="steps-post")
ax.legend()
plt.show()

index_best_model = np.argmax(validate_scores)
best_model = clfs[index_best_model]
print('Training accuracy of best model: ',best_model.score(X_train, y_train))
print('validate accuracy of best model: ',best_model.score(X_validate, y_validate))

#-------------------------------------------------------------------------------------------------------------------Q2-b . 2

ds = pd.read_csv('train.csv')
y = np.array(ds['target'])
y = y[:, np.newaxis]
ds = ds.drop(columns=['target'])
x = np.array(ds)
X_test = x[2500:5000, :]
Y_test = y[2500:5000, :]
X_train =ds.drop(ds.index[2500:5000])
Y_train = np.delete(y, range(2500,5000))


X_validate=pd.DataFrame(X_test)
X_validate = pd.get_dummies(X_validate)  
y_validate=pd.DataFrame(Y_test)
y_validate = pd.get_dummies(y_validate)  
X_train=pd.DataFrame(X_train)
X_train = pd.get_dummies(X_train)  
Y_train=pd.DataFrame(Y_train)
Y_train = pd.get_dummies(Y_train)  
X_validate,y_validate , X_train , Y_train = X_validate.fillna(0),y_validate.fillna(0),X_train.fillna(0) ,Y_train.fillna(0)

y_validate= y_validate.iloc[:, :-1]
Y_train= Y_train.iloc[:, :-1]
X_train= X_train.iloc[:, :-6]
X_train.shape

y_validate.head

print(X_validate.shape ,y_validate.shape ,X_train.shape ,Y_train.shape )

X_validate=np.argmax(X_validate, axis=1)
y_validate=np.argmax(y_validate, axis=1)

model = DecisionTreeClassifier(random_state=1).fit(X_train,y_train)
y_predicted = model.predict(X_validate)
print('Training accuracy: ',model.score(X_train,y_train))
#print('validation Accuracy: ',model.score(X_validate,y_validate))

path=DecisionTreeClassifier(random_state=1).\
cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")

clfs = []
for ccp_alpha in ccp_alphas:
  if(ccp_alpha >= 0):
    clf = DecisionTreeClassifier(random_state=1,ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

print("Number of nodes in the last tree is: {} with ccp_alpha: {} and a depth of: {}".format(
clfs[-1].tree_.node_count, ccp_alphas[-1],clfs[-1].tree_.max_depth))

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]
node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

train_scores = [clf.score(X_train, y_train) for clf in clfs]
#validate_scores = [clf.score(int(X_validate),int(y_validate)) for clf in clfs]
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and validateing sets")
ax.plot(ccp_alphas[:-1], train_scores, marker='o', label="train",
        drawstyle="steps-post")
#ax.plot(ccp_alphas, validate_scores, marker='o', label="validate",
 #       drawstyle="steps-post")
ax.legend()
plt.show()

index_best_model = np.argmax(validate_scores)
best_model = clfs[index_best_model]
print('Training accuracy of best model: ',best_model.score(X_train, y_train))
#print('validate accuracy of best model: ',best_model.score(X_validate, y_validate))

#----------------------------------------------------------------------------------------------------------------------Q2-b .3

ds = pd.read_csv('train.csv')
y = np.array(ds['target'])
y = y[:, np.newaxis]
ds = ds.drop(columns=['target'])
x = np.array(ds)
X_test = x[5000:7500, :]
Y_test = y[5000:7500, :]
X_train =ds.drop(ds.index[5000:7500])
Y_train = np.delete(y, range(5000,7500))

X_validate=pd.DataFrame(X_test)
X_validate = pd.get_dummies(X_validate)  
y_validate=pd.DataFrame(Y_test)
y_validate = pd.get_dummies(y_validate)  
X_train=pd.DataFrame(X_train)
X_train = pd.get_dummies(X_train)  
Y_train=pd.DataFrame(Y_train)
Y_train = pd.get_dummies(Y_train)

y_validate= y_validate.iloc[:, :-1]
Y_train= Y_train.iloc[:, :-1]
X_train= X_train.iloc[:, :-6]
X_train.shape

print(X_validate.shape ,y_validate.shape ,X_train.shape ,Y_train.shape )

model = DecisionTreeClassifier(random_state=1).fit(X_train,y_train)
y_predicted = model.predict(X_validate)
print('Training accuracy: ',model.score(X_train,y_train))
#print('validation Accuracy: ',model.score(X_validate,y_validate))

path=DecisionTreeClassifier(random_state=1).\
cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")

clfs = []
for ccp_alpha in ccp_alphas:
  if(ccp_alpha >= 0):
    clf = DecisionTreeClassifier(random_state=1,ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

print("Number of nodes in the last tree is: {} with ccp_alpha: {} and a depth of: {}".format(
clfs[-1].tree_.node_count, ccp_alphas[-1],clfs[-1].tree_.max_depth))

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]
node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

train_scores = [clf.score(X_train, y_train) for clf in clfs]
#validate_scores = [clf.score(int(X_validate),int(y_validate)) for clf in clfs]
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and validateing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train",
        drawstyle="steps-post")
#ax.plot(ccp_alphas, validate_scores, marker='o', label="validate",
 #       drawstyle="steps-post")
ax.legend()
plt.show()

index_best_model = np.argmax(validate_scores)
best_model = clfs[index_best_model]
print('Training accuracy of best model: ',best_model.score(X_train, y_train))
#print('validate accuracy of best model: ',best_model.score(X_validate, y_validate))

#-------------------------------------------------------------------------------------------------------------------Q2-b .4

ds = pd.read_csv('train.csv')
y = np.array(ds['target'])
y = y[:, np.newaxis]
ds = ds.drop(columns=['target'])
x = np.array(ds)
X_test = x[7500:, :]
Y_test = y[7500:, :]
X_train =ds.drop(ds.index[7500:])
Y_train = np.delete(y, range(7500,10000))

X_validate=pd.DataFrame(X_test)
X_validate = pd.get_dummies(X_validate)  
y_validate=pd.DataFrame(Y_test)
y_validate = pd.get_dummies(y_validate)  
X_train=pd.DataFrame(X_train)
X_train = pd.get_dummies(X_train)  
Y_train=pd.DataFrame(Y_train)
Y_train = pd.get_dummies(Y_train)  
#_, X_test, _, y_test = train_test_split(X_test, y_test, test_size=1,shuffle = False)

y_validate= y_validate.iloc[:, :-1]
Y_train= Y_train.iloc[:, :-1]
X_train= X_train.iloc[:, :-5]
X_train.shape

print(X_validate.shape ,y_validate.shape ,X_train.shape ,Y_train.shape )

model = DecisionTreeClassifier(random_state=1).fit(X_train,y_train)
y_predicted = model.predict(X_validate)
print('Training accuracy: ',model.score(X_train,y_train))
#print('validation Accuracy: ',model.score(X_validate,y_validate))

path=DecisionTreeClassifier(random_state=1).\
cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")

clfs = []
for ccp_alpha in ccp_alphas:
  if(ccp_alpha >= 0):
    clf = DecisionTreeClassifier(random_state=1,ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

print("Number of nodes in the last tree is: {} with ccp_alpha: {} and a depth of: {}".format(
clfs[-1].tree_.node_count, ccp_alphas[-1],clfs[-1].tree_.max_depth))

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]
node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas[:-1], node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas[:-1], depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

train_scores = [clf.score(X_train, y_train) for clf in clfs]
#validate_scores = [clf.score(int(X_validate),int(y_validate)) for clf in clfs]
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and validateing sets")
ax.plot(ccp_alphas[:-1], train_scores, marker='o', label="train",
        drawstyle="steps-post")
#ax.plot(ccp_alphas, validate_scores, marker='o', label="validate",
 #       drawstyle="steps-post")
ax.legend()
plt.show()

index_best_model = np.argmax(validate_scores)
best_model = clfs[index_best_model]
print('Training accuracy of best model: ',best_model.score(X_train, y_train))
#print('validate accuracy of best model: ',best_model.score(X_validate, y_validate))

#---------------------------------------------------------------------------------------------------------------------Q2-c

data = pd.read_csv('train.csv')
test = pd.read_csv('adult.test.10k.csv')
test = test.sample(frac=0.25)
X,y = data[[ 'workclass',  'education', 'marital-status', 'occupation','relationship',  'race','sex','native-country']],data['target']
X_test,y_test = test[[ 'workclass',  'education', 'marital-status', 'occupation','relationship',  'race','sex','native-country']],test['target']
X,y = X.fillna(0),y.fillna(0)
X = pd.get_dummies(X)    
_, X_test, _, y_test = train_test_split(X_test, y_test, test_size=0.25,shuffle = True)

y_test.head

model = DecisionTreeClassifier(random_state=1).fit(X_train,y_train)
y_predicted = model.predict(X_validate)
print('Training accuracy: ',model.score(X_train,y_train))
print('validation Accuracy: ',model.score(X_validate,y_validate))

path=DecisionTreeClassifier(random_state=1).\
cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")

clfs = []
for ccp_alpha in ccp_alphas:
  if(ccp_alpha >= 0):
    clf = DecisionTreeClassifier(random_state=1,ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

print("Number of nodes in the last tree is: {} with ccp_alpha: {} and a depth of: {}".format(
clfs[-1].tree_.node_count, ccp_alphas[-1],clfs[-1].tree_.max_depth))

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]
node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1)
ax[0].plot(ccp_alphas[:-1], node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas[:-1], depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

train_scores = [clf.score(X_train, y_train) for clf in clfs]
validate_scores = [clf.score(X_validate,y_validate) for clf in clfs]
fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and validateing sets")
ax.plot(ccp_alphas[:-1], train_scores, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas[:-1], validate_scores, marker='o', label="validate",
        drawstyle="steps-post")
ax.legend()
plt.show()

index_best_model = np.argmax(validate_scores)
best_model = clfs[index_best_model]
print('Training accuracy of best model: ',best_model.score(X_train, y_train))
print('validate accuracy of best model: ',best_model.score(X_validate, y_validate))